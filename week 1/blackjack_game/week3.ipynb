{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57860fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from blackjack import BlackjackGame # Importing your Week 1 code\n",
    "\n",
    "class BlackjackEnv:\n",
    "    def __init__(self):\n",
    "        self.game = BlackjackGame()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Returns (PlayerSum, DealerUpcard, UsableAce)\"\"\"\n",
    "        # Dealer's second card is the 'upcard' in your Week 1 setup\n",
    "        dealer_card = self.game.dealer_hand.cards[1]\n",
    "        \n",
    "        # Convert rank to numeric value\n",
    "        if dealer_card.rank in ['J', 'Q', 'K']:\n",
    "            d_val = 10\n",
    "        elif dealer_card.rank == 'A':\n",
    "            d_val = 11\n",
    "        else:\n",
    "            d_val = int(dealer_card.rank)\n",
    "            \n",
    "        # Check for usable ace: player has an Ace and hand <= 21\n",
    "        usable_ace = any(c.rank == 'A' for c in self.game.player_hand.cards) and self.game.player_hand.value <= 21\n",
    "        return (self.game.player_hand.value, d_val, usable_ace)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Starts a new round and returns initial state\"\"\"\n",
    "        self.game.deal_initial()\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Action: 0 = Stand, 1 = Hit\n",
    "        Returns: (state, reward, done)\n",
    "        \"\"\"\n",
    "        if action == 1: # HIT\n",
    "            self.game.player_hand.add_card(self.game.deck.draw())\n",
    "            if self.game.player_hand.value > 21:\n",
    "                return self._get_state(), -1, True # Lose\n",
    "            return self._get_state(), 0, False # Game continues\n",
    "        \n",
    "        else: # STAND\n",
    "            # Dealer plays: hits until 17\n",
    "            while self.game.dealer_hand.value < 17:\n",
    "                self.game.dealer_hand.add_card(self.game.deck.draw())\n",
    "            \n",
    "            p_val = self.game.player_hand.value\n",
    "            d_val = self.game.dealer_hand.value\n",
    "            \n",
    "            if d_val > 21 or p_val > d_val:\n",
    "                reward = 1\n",
    "            elif d_val > p_val:\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 0\n",
    "            return self._get_state(), reward, True\n",
    "\n",
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_1_prediction(n_episodes=10000):\n",
    "    # Dictionary to store total returns and counts for averaging\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    V = defaultdict(float)\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        done = False\n",
    "        \n",
    "        # 1. Generate one full game\n",
    "        while not done:\n",
    "            action = 1 if state[0] < 20 else 0 # Policy: Hit < 20\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append((state, reward))\n",
    "            state = next_state\n",
    "            \n",
    "        # 2. Process the results (First-Visit)\n",
    "        G = episode[-1][1] # The final win/loss result\n",
    "        visited_states = set()\n",
    "        for s, _ in episode:\n",
    "            if s not in visited_states:\n",
    "                returns_sum[s] += G\n",
    "                returns_count[s] += 1\n",
    "                V[s] = returns_sum[s] / returns_count[s]\n",
    "                visited_states.add(s)\n",
    "                \n",
    "    # Results\n",
    "    print(f\"--- Task 1 Results ---\")\n",
    "    print(f\"Value of State (Player 21, Dealer 10): {V.get((21, 10, False), 0):.4f}\")\n",
    "    print(f\"Value of State (Player 5, Dealer 10): {V.get((5, 10, False), 0):.4f}\")\n",
    "    return V\n",
    "\n",
    "# Run it\n",
    "v_table = task_1_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c4552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_2_control(n_episodes=500000, alpha=0.02, epsilon=0.1):\n",
    "    # Q[(state)][action] -> [Value of Stand, Value of Hit]\n",
    "    Q = defaultdict(lambda: np.zeros(2))\n",
    "    reward_history = []\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        done = False\n",
    "        \n",
    "        # 1. Play using Epsilon-Greedy\n",
    "        while not done:\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.choice([0, 1]) # Explore\n",
    "            else:\n",
    "                action = np.argmax(Q[state]) # Exploit\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append((state, action))\n",
    "            state = next_state\n",
    "            \n",
    "        # 2. Update Q-table at the end of the round\n",
    "        G = reward # The final result\n",
    "        for s, a in episode:\n",
    "            # Incremental Mean update\n",
    "            Q[s][a] += alpha * (G - Q[s][a])\n",
    "        \n",
    "        reward_history.append(G)\n",
    "        \n",
    "        if i % 100000 == 0:\n",
    "            print(f\"Simulation Progress: {i}/{n_episodes} games...\")\n",
    "\n",
    "    return Q, reward_history\n",
    "\n",
    "# Run the training\n",
    "Q_optimal, history = task_2_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ddd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history, Q):\n",
    "    # 1. Rolling Average Reward\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    window = 2000\n",
    "    rolling_avg = np.convolve(history, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(rolling_avg, color='green')\n",
    "    plt.title(\"Learning Curve: Rolling Average Reward\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Reward (Average over 2000 games)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Strategy Card Heatmap (No Usable Ace)\n",
    "    # We want to show Player Sum (12-21) vs Dealer Card (2-11)\n",
    "    strategy = np.zeros((10, 10))\n",
    "    for p_idx, p_sum in enumerate(range(12, 22)):\n",
    "        for d_idx, d_card in enumerate(range(2, 12)):\n",
    "            # Pick the best action (0 or 1) from our Q-table\n",
    "            strategy[p_idx, d_idx] = np.argmax(Q[(p_sum, d_card, False)])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(strategy, annot=True, cmap=\"coolwarm\", \n",
    "                xticklabels=range(2, 12), yticklabels=range(12, 22))\n",
    "    plt.title(\"Optimal Strategy Heatmap (0=Stand, 1=Hit)\")\n",
    "    plt.xlabel(\"Dealer Upcard Value\")\n",
    "    plt.ylabel(\"Player Sum\")\n",
    "    plt.show()\n",
    "\n",
    "plot_results(history, Q_optimal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
